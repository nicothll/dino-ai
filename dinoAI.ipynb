{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSS used for screen capture in MacOS\n",
    "from mss.darwin import MSS as mss\n",
    "# Sending commands\n",
    "import pyautogui\n",
    "# OpenCV allows su to do frame processing\n",
    "import cv2\n",
    "# Transformational framework\n",
    "import numpy as np\n",
    "# OCR for game over extraction\n",
    "import pytesseract\n",
    "# Visualize capture frames\n",
    "import matplotlib.pyplot as plt\n",
    "# Bring in time for pauses\n",
    "import time\n",
    "# Environment components\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGame(Env):\n",
    "    # Setup the environment action and observation shapes\n",
    "    def __init__(self) -> None:\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "        # Setup spaces\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=255, shape=(1, 40, 90), dtype=np.uint8\n",
    "        )\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        # Capture game frames\n",
    "        self.game_location = {\"top\": 300, \"left\": 75, \"width\": 400, \"height\": 100}\n",
    "        self.done_location = {\"top\": 270, \"left\": 275, \"width\": 100, \"height\": 70}\n",
    "\n",
    "    # What is called to do something in the game\n",
    "    def step(self, action):\n",
    "        # Action key - 0 = SpaceBar(Jump), 1 = Duck(down), 2 = No Action(no op)\n",
    "        # action_map = {0: \"up\", 1: \"down\", 2: \"no_op\"}\n",
    "        action_map = {0: \"up\", 1: \"no_op\"}\n",
    "        if action != 2:\n",
    "            pyautogui.press(action_map[action])\n",
    "\n",
    "        # Checking whether the game is done\n",
    "        done, done_cap = self.get_done()\n",
    "        # Get the next observation\n",
    "        new_obs = self.get_observation()\n",
    "        # Reward - We get a point for every frame we're alive\n",
    "        reward = 1\n",
    "        # Info dictionary\n",
    "        info = {}\n",
    "\n",
    "        return new_obs, reward, done, info\n",
    "\n",
    "    # Visualize the game\n",
    "    def render(self, mode=\"human\"):\n",
    "        with mss() as sct:\n",
    "            cv2.imshow(\"Game\", np.array(sct.grab(self.game_location))[:, :, :3])\n",
    "            if cv2.waitKey(1) and 0xFF == ord(\"q\"):\n",
    "                self.close()\n",
    "\n",
    "    # Restart the game\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        pyautogui.click(x=150, y=200)\n",
    "        pyautogui.press(\"up\")\n",
    "        return self.get_observation()\n",
    "\n",
    "    # This closes down the observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Get the part of the observation of the game that we want\n",
    "    def get_observation(self):\n",
    "        with mss() as sct:\n",
    "            # Get screen capture of game\n",
    "            raw = np.array(sct.grab(self.game_location))[:, :, :3]\n",
    "            # Grayscale\n",
    "            gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "            # Resize\n",
    "            resized = cv2.resize(gray, (90, 40))\n",
    "            # Add channels first\n",
    "            channel = np.reshape(resized, (1, 40, 90))\n",
    "            return channel\n",
    "\n",
    "    # Get the done text using OCR\n",
    "    def get_done(self):\n",
    "        # Get done screen\n",
    "        with mss() as sct:\n",
    "            done_cap = np.array(sct.grab(self.done_location))[:, :, :3]\n",
    "            done_strings = [\"GAME\", \"GAHE\"]\n",
    "            # Apply OCR\n",
    "            done = False\n",
    "            result = pytesseract.image_to_string(done_cap)[:4]\n",
    "            if result in done_strings:\n",
    "                done = True\n",
    "            return done, done_cap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env.get_observation()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done, done_cap = env.get_done()\n",
    "print(done)\n",
    "plt.imshow(done_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Play 10 games\n",
    "# for episode in range(10):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         obs, reward, done, info = env.step(env.action_space.sample())\n",
    "#         total_reward += reward\n",
    "#     print(f\"Total Reward for episode {episode} is {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os\n",
    "# IMport Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "# Check Environment\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the environment is okay\n",
    "env = WebGame()\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly use in all RL\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose: int = 1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, f\"best_model_{self.n_calls}\")\n",
    "            self.model.save(model_path)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build DQN and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DQN algorithm\n",
    "from stable_baselines3.dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN Model\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    tensorboard_log=LOG_DIR,\n",
    "    verbose=1,\n",
    "    buffer_size=180000, # Depends of your RAM\n",
    "    learning_starts=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off training\n",
    "model.learn(total_timesteps=10000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test Out Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model, inform your best model in @best_model without .zip\n",
    "best_model = \"best_model_8000\"\n",
    "model.load(os.path.join(CHECKPOINT_DIR, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play 10 games with our trained model\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        total_reward += reward\n",
    "    print(f\"Total Reward for episode {episode} is {total_reward}\")\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a616d960eba5edbc548fd92c67d9696fe61c09d798b4c9d49b99ddbbac72abe1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
